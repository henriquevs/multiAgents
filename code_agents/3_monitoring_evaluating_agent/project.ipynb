{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a588c60d-f312-411c-95c7-3115c5163749",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <tr>\n",
    "    <td><img src=\"fleet.png\" alt=\"fleet of icecream trucks\" width=\"120\"/></td>\n",
    "    <td align=\"left\"><h1>Lesson 4: Monitoring and Evaluating your Agent</h1></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0d2d7-3657-4559-ba64-b7c7f6c144bb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb0e8d-4654-4241-ae3d-001e8dee07d8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703bfe5-3999-4caa-9023-a3d6f0b22bc6",
   "metadata": {},
   "source": [
    "## Setup Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9bffc7-51df-4763-8871-5c68e4c267b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME=\"Customer-Success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6b099c-b46c-499d-b202-020827b3bda9",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: Customer-Success\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://localhost:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n",
    "\n",
    "os.environ.setdefault(\n",
    "    \"PHOENIX_COLLECTOR_ENDPOINT\",\n",
    "    os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\", \"http://localhost:6006\")\n",
    ")  # phoenix.otel will read this env var if endpoint arg is omitted :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=PROJECT_NAME,\n",
    "    # endpoint= get_phoenix_endpoint() + \"v1/traces\"\n",
    "    # endpoint = os.getenv('DLAI_LOCAL_URL').format(port='6006') + \"v1/traces\"\n",
    "    endpoint = os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"].rstrip(\"/\") + \"/v1/traces\"\n",
    ")\n",
    "SmolagentsInstrumentor().instrument(tracer_provider=tracer_provider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c29038-093f-43d4-90d6-d0d7d0e3d189",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-681ba102-7a90333d6ce5a9786828e0fc;3e68fdb4-921e-4e10-bb30-a6b3de9c5b06)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msmolagents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApiModel\n\u001b[32m      3\u001b[39m model=HfApiModel(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-Coder-32B-Instruct\u001b[39m\u001b[33m\"\u001b[39m, provider=\u001b[33m\"\u001b[39m\u001b[33mtogether\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/openinference/instrumentation/smolagents/_wrappers.py:287\u001b[39m, in \u001b[36m_ModelWrapper.__call__\u001b[39m\u001b[34m(self, wrapped, instance, args, kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m model = instance\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tracer.start_as_current_span(\n\u001b[32m    278\u001b[39m     span_name,\n\u001b[32m    279\u001b[39m     attributes={\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m     },\n\u001b[32m    286\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     output_message = \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     span.set_status(trace_api.StatusCode.OK)\n\u001b[32m    289\u001b[39m     span.set_attribute(LLM_TOKEN_COUNT_PROMPT, model.last_input_token_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/smolagents/models.py:1009\u001b[39m, in \u001b[36mHfApiModel.__call__\u001b[39m\u001b[34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    993\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    994\u001b[39m     messages: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    998\u001b[39m     **kwargs,\n\u001b[32m    999\u001b[39m ) -> ChatMessage:\n\u001b[32m   1000\u001b[39m     completion_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_completion_kwargs(\n\u001b[32m   1001\u001b[39m         messages=messages,\n\u001b[32m   1002\u001b[39m         stop_sequences=stop_sequences,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         **kwargs,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_input_token_count = response.usage.prompt_tokens\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_output_token_count = response.usage.completion_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:992\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    964\u001b[39m parameters = {\n\u001b[32m    965\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    966\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    983\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    984\u001b[39m }\n\u001b[32m    985\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    986\u001b[39m     inputs=messages,\n\u001b[32m    987\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    991\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/codeAgents/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-681ba102-7a90333d6ce5a9786828e0fc;3e68fdb4-921e-4e10-bb30-a6b3de9c5b06)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "from smolagents import HfApiModel\n",
    "\n",
    "model=HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"together\")\n",
    "\n",
    "model([{\"role\": \"user\", \"content\": \"Hello!\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d53b9d-3fb8-4e4e-b303-a0d9c6bd75ba",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# This is where you can access the display:\n",
    "# print(os.environ.get('DLAI_LOCAL_URL').format(port='6006'))\n",
    "print(os.environ.get('PHOENIX_COLLECTOR_ENDPOINT').format(port='6006') + '/projects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac68f6-7370-409d-bf5d-9b83d54ad9f5",
   "metadata": {},
   "source": [
    "## Trace an agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f777d52-0a95-48a6-9e76-4491fa52840d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from smolagents import HfApiModel, CodeAgent\n",
    "\n",
    "agent = CodeAgent(model=model, tools=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8d8c3-98d0-405c-b79f-2754565ae417",
   "metadata": {},
   "source": [
    ">Note, the following line will sometimes get a timeout on the interface to the tracing package due to the networked interface. If this happens, try it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cccbd1-924d-4c9e-bac3-7ab1d8e24971",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "agent.run(\"What is the 100th Fibonacci number?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d11d7a-9950-47fe-affc-b4fa4f2032a6",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# This is where you can access the display:\n",
    "print(os.environ.get('DLAI_LOCAL_URL').format(port='6006'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c492d21-31a6-4345-909c-f5481e5054dc",
   "metadata": {},
   "source": [
    "## Setup ice cream production system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d06321-fecc-4b6f-9193-c50f12fe0cd1",
   "metadata": {
    "height": 589
   },
   "outputs": [],
   "source": [
    "from smolagents import tool\n",
    "from typing import Dict\n",
    "\n",
    "menu_prices = {\"crepe nutella\": 1.50, \"vanilla ice cream\": 2, \"maple pancake\": 1.}\n",
    "\n",
    "ORDER_BOOK = {}\n",
    "\n",
    "@tool\n",
    "def place_order(quantities: Dict[str, int], session_id: int) -> None:\n",
    "    \"\"\"Places a pre-order of snacks.\n",
    "\n",
    "    Args:\n",
    "        quantities: a dictionary with names as keys and quantities as values\n",
    "        session_id: the id for the client session\n",
    "    \"\"\"\n",
    "    global ORDER_BOOK\n",
    "    assert isinstance(quantities, dict), \"Incorrect type for the input dictionary!\"\n",
    "    assert [key in menu_prices for key in quantities.keys()], f\"All food names should be within {menu_prices.keys()}\"\n",
    "    ORDER_BOOK[session_id] = quantities\n",
    "\n",
    "@tool\n",
    "def get_prices(quantities: Dict[str, int]) -> str:\n",
    "    \"\"\"Gets price for certain quantities of ice cream.\n",
    "\n",
    "    Args:\n",
    "        quantities: a dictionary with names as keys and quantities as values\n",
    "    \"\"\"\n",
    "    assert isinstance(quantities, dict), \"Incorrect type for the input dictionary!\"\n",
    "    assert [key in menu_prices for key in quantities.keys()], f\"All food names should be within {menu_prices.keys()}\"\n",
    "    total_price = sum([menu_prices[key] * value for key, value in quantities.items()])\n",
    "    return (\n",
    "        f\"Given the current menu prices:\\n{menu_prices}\\nThe total price for your order would be: ${total_price}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99a5bf-68e5-4cad-a13c-9a99c244a064",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "order_agent = CodeAgent(\n",
    "    tools=[place_order, get_prices],\n",
    "    model=HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"together\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b50f2-cfb6-4ad1-8582-608c7744328a",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "order_agent.run(\n",
    "    \"Could I come and collect one crepe nutella?\",\n",
    "    additional_args={\"session_id\": 192}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58c530-26ff-4413-a5a6-0600c784b508",
   "metadata": {},
   "source": [
    "### Try multiple orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ead873-f77f-44f9-b87a-9dbbbe32bf81",
   "metadata": {
    "height": 147
   },
   "outputs": [],
   "source": [
    "client_requests = [\n",
    "    (\"Could I come and collect one crepe nutella?\", \"place_order\"),\n",
    "    (\"What would be the price for 1 crÃªpe nutella + 2 pancakes?\", \"get_prices\"),\n",
    "    (\"How did you start your ice-cream business?\", None),\n",
    "    (\"What's the weather at the Louvre right now?\", None),\n",
    "    (\"I'm not sure if I should order. I want a vanilla ice cream. but if it's more expensive than $1, I don't want it. If it's below, I'll order it, please.\", \"place_order\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb4e26-3694-47bb-a2f1-99baccae5dbb",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "for request in client_requests:\n",
    "    order_agent.run(\n",
    "        request[0],\n",
    "        additional_args={\"session_id\": 0, \"menu_prices\": menu_prices}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830e451-9e91-49c8-acdf-df9044a9c45e",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "spans = px.Client().get_spans_dataframe(project_name=PROJECT_NAME)\n",
    "spans.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1336b-1e55-46b6-ab1c-26dfcf8355d5",
   "metadata": {},
   "source": [
    "### Add processing to extract desired information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c56b96-1d6e-43ca-9664-9f998936ca34",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "agents = spans[spans['span_kind'] == 'AGENT'].copy()\n",
    "agents['task'] = agents['attributes.input.value'].apply(\n",
    "    lambda x: json.loads(x).get('task') if isinstance(x, str) else None\n",
    ")\n",
    "\n",
    "tools = spans.loc[\n",
    "    spans['span_kind'] == 'TOOL',\n",
    "    [\"attributes.tool.name\", \"attributes.input.value\", \"context.trace_id\"]\n",
    "].copy()\n",
    "\n",
    "tools_per_task = agents[\n",
    "    [\"name\", \"start_time\", \"task\", \"context.trace_id\"]\n",
    "].merge(\n",
    "    tools,\n",
    "    on=\"context.trace_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "tools_per_task.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d6117-87fe-476c-81fe-294a6e6a6f32",
   "metadata": {},
   "source": [
    "### Now, compare tool calls with exected tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c21f57-e5f6-45ba-987b-10fab042bb3e",
   "metadata": {
    "height": 317
   },
   "outputs": [],
   "source": [
    "def score_request(expected_tool: str, tool_calls: list):\n",
    "    if expected_tool is None:\n",
    "        return tool_calls == set([\"final_answer\"])\n",
    "    else:\n",
    "        return expected_tool in tool_calls\n",
    "\n",
    "results = []\n",
    "for request, expected_tool in client_requests:\n",
    "    tool_calls = set(tools_per_task.loc[tools_per_task[\"task\"] == request, \"attributes.tool.name\"].tolist())\n",
    "    results.append(\n",
    "        {\n",
    "            \"request\": request,\n",
    "            \"tool_calls_performed\": tool_calls,\n",
    "            \"is_correct\": score_request(expected_tool, tool_calls)\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f597dc-0e43-48f9-9780-2c14675e2fd9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeAgents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
